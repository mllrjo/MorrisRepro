"""
File: test_scaled_pipeline.py
Directory: memorization_reproduction/

Scaled-up Morris et al. memorization reproduction pipeline test.
Progressive scaling from 10M to 100M+ parameter models to approach Morris targets.
"""

import os
import sys
import time
import traceback
import psutil
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple

# Add src directory to path for imports
src_path = os.path.join(os.path.dirname(__file__), 'src')
sys.path.insert(0, src_path)


def get_system_resources() -> Dict[str, Any]:
    """Get current system resource information."""
    return {
        'cpu_count': psutil.cpu_count(),
        'memory_gb': psutil.virtual_memory().total / (1024**3),
        'memory_available_gb': psutil.virtual_memory().available / (1024**3),
        'memory_percent': psutil.virtual_memory().percent
    }


def detect_device_and_constraints() -> Tuple[str, Dict[str, Any]]:
    """Detect device and estimate memory constraints."""
    print("=" * 60)
    print("DEVICE AND RESOURCE DETECTION")
    print("=" * 60)
    
    try:
        import torch
        
        # Device detection
        if torch.cuda.is_available():
            device = "cuda"
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"✓ CUDA available: {torch.cuda.get_device_name()}")
            print(f"  GPU Memory: {gpu_memory:.1f} GB")
            
            # Estimate max model size based on GPU memory
            # Rule of thumb: ~4 bytes per parameter for float32, plus overhead
            max_params_gpu = int(gpu_memory * 0.7 * 1e9 / 8)  # Conservative estimate
            
        else:
            device = "cpu"
            gpu_memory = 0
            max_params_gpu = 0
            print("✓ Using CPU (CUDA not available)")
        
        # System memory
        sys_resources = get_system_resources()
        available_memory = sys_resources['memory_available_gb']
        
        print(f"System Memory: {sys_resources['memory_gb']:.1f} GB total, {available_memory:.1f} GB available")
        print(f"CPU Cores: {sys_resources['cpu_count']}")
        
        # Estimate max model size for CPU (more conservative)
        max_params_cpu = int(available_memory * 0.3 * 1e9 / 8)  # Very conservative
        
        constraints = {
            'device': device,
            'gpu_memory_gb': gpu_memory,
            'cpu_memory_available_gb': available_memory,
            'max_params_gpu': max_params_gpu,
            'max_params_cpu': max_params_cpu,
            'recommended_max_params': max_params_gpu if device == "cuda" else max_params_cpu
        }
        
        print(f"Estimated max parameters: {constraints['recommended_max_params']:,}")
        
        return device, constraints
        
    except ImportError:
        device = "cpu"
        sys_resources = get_system_resources()
        constraints = {
            'device': device,
            'gpu_memory_gb': 0,
            'cpu_memory_available_gb': sys_resources['memory_available_gb'],
            'max_params_gpu': 0,
            'max_params_cpu': int(sys_resources['memory_available_gb'] * 0.3 * 1e9 / 8),
            'recommended_max_params': int(sys_resources['memory_available_gb'] * 0.3 * 1e9 / 8)
        }
        return device, constraints


def create_progressive_model_configs(constraints: Dict[str, Any]) -> List[Tuple[Any, str, int]]:
    """Create progressively larger model configurations based on system constraints."""
    print("\n" + "=" * 60)
    print("CREATING PROGRESSIVE MODEL CONFIGURATIONS")
    print("=" * 60)
    
    # Simple config classes
    class ScaledModelConfig:
        def __init__(self, name, n_layers, d_model, n_heads, vocab_size, max_seq_length, dropout=0.1):
            self.name = name
            self.n_layers = n_layers
            self.d_model = d_model
            self.n_heads = n_heads
            self.vocab_size = vocab_size
            self.max_seq_length = max_seq_length
            self.dropout = dropout
            
        def estimate_parameters(self):
            """Rough parameter count estimation for transformer."""
            # Embedding: vocab_size * d_model
            # Position embedding: max_seq_length * d_model  
            # Transformer blocks: n_layers * (4 * d_model^2 + layer_norm params)
            # Output projection: d_model * vocab_size
            
            embed_params = self.vocab_size * self.d_model
            pos_embed_params = self.max_seq_length * self.d_model
            
            # Per layer: attention (4 * d_model^2) + FFN (8 * d_model^2) + layer norms
            layer_params = self.n_layers * (12 * self.d_model ** 2 + 4 * self.d_model)
            
            output_params = self.d_model * self.vocab_size
            
            total = embed_params + pos_embed_params + layer_params + output_params
            return total
    
    # Define progressive model configurations - start smaller for constrained systems
    # Define progressive model configurations - DEBUG SIZES
    model_configs = [
        # Ultra-small models for debugging memorization mechanism
        ("Debug-Micro", 1, 32, 2, 256, 32),    # ~8K parameters
        ("Debug-Mini", 1, 64, 4, 512, 32),     # ~35K parameters  
        ("Debug-Small", 2, 64, 4, 512, 32),    # ~70K parameters
        ("Debug-Medium", 2, 128, 8, 1024, 64), # ~280K parameters
    ]
#    model_configs = [
#        # Tiny (for very constrained systems)
#        ("Tiny", 2, 128, 4, 1000, 64),
#        
#        # Mini (slightly larger)
#        ("Mini", 3, 192, 6, 1500, 96),
#        
#        # Small (baseline)
#        ("Small", 4, 256, 8, 2048, 128),
#        
#        # Medium (~10M parameters)
#        ("Medium", 6, 512, 8, 2048, 128),
#        
#        # Large (~50M parameters) 
#        ("Large", 12, 768, 12, 4096, 128),
#        
#        # Extra Large (~100M parameters)
#        ("XL", 18, 1024, 16, 4096, 128),
#        
#        # XXL (~200M parameters)
#        ("XXL", 24, 1280, 20, 8192, 128),
#    ]
    
    # Filter configs based on system constraints
    max_params = constraints['recommended_max_params']
    viable_configs = []
    
    print(f"System constraint: max {max_params:,} parameters")
    print("\nEvaluating model configurations:")
    
    for name, n_layers, d_model, n_heads, vocab_size, max_seq_length in model_configs:
        config = ScaledModelConfig(name, n_layers, d_model, n_heads, vocab_size, max_seq_length)
        estimated_params = config.estimate_parameters()
        
        if estimated_params <= max_params:
            viable_configs.append((config, name, estimated_params))
            status = "✓ VIABLE"
        else:
            status = "✗ TOO LARGE"
            
        print(f"  {name:8} ({n_layers:2}L, {d_model:4}D): ~{estimated_params:8,} params {status}")
        
        # Include at least the first viable config even if it's close to limit
        if len(viable_configs) == 0 and estimated_params <= max_params * 1.2:
            viable_configs.append((config, name, estimated_params))
            print(f"    → Including {name} despite tight constraints")
    
    if not viable_configs:
        # Fallback to ultra-minimal config for very constrained systems
        print("\n⚠ No viable configs found, creating ultra-minimal fallback")
        fallback = ScaledModelConfig("Ultra-Mini", 1, 64, 2, 500, 32)
        viable_configs.append((fallback, "Ultra-Mini", fallback.estimate_parameters()))
    
    print(f"\nSelected {len(viable_configs)} viable configurations")
    return viable_configs

def create_scaled_training_config(model_size: str, device: str) -> Any:
    """EXTREME memorization training - get loss from 6.9 → 0.1"""
    
    class ScaledTrainingConfig:
        def __init__(self, batch_size, learning_rate, max_steps, warmup_steps, weight_decay=0.01):
            self.batch_size = batch_size
            self.learning_rate = learning_rate
            self.max_steps = max_steps
            self.warmup_steps = warmup_steps
            self.weight_decay = weight_decay
    
    # EXTREME settings for random sequence memorization  
    config_map = {
        "Tiny": {"batch_size": 64, "max_steps": 25000, "lr": 2e-2},
        "Mini": {"batch_size": 64, "max_steps": 30000, "lr": 1.5e-2}, 
        "Small": {"batch_size": 64, "max_steps": 35000, "lr": 1e-2},
        "Medium": {"batch_size": 64, "max_steps": 40000, "lr": 8e-3},
        "Large": {"batch_size": 64, "max_steps": 45000, "lr": 5e-3},
        "XL": {"batch_size": 64, "max_steps": 50000, "lr": 3e-3},
        "XXL": {"batch_size": 64, "max_steps": 55000, "lr": 2e-3},
    }
    
    params = config_map.get(model_size, config_map["Small"])
    
    # Minimal weight decay - we WANT overfitting for memorization
    weight_decay = 0.0001  # Almost no regularization
    
    # Adjust for device 
    if device == "cpu":
        params["batch_size"] = max(32, params["batch_size"] // 2)  
        params["max_steps"] = max(15000, params["max_steps"] // 2)  # Still very aggressive
    
    return ScaledTrainingConfig(
        batch_size=params["batch_size"],
        learning_rate=params["lr"],
        max_steps=params["max_steps"],
        warmup_steps=params["max_steps"] // 100,  # Tiny warmup - get to high LR immediately
        weight_decay=weight_decay
    )

    def get_scaled_dataset_sizes(model_size: str) -> List[int]:
        """Dataset sizes appropriate for model capacity."""
        size_map = {
            "Debug-Micro": [50, 100, 200, 400, 800],      # Tiny capacity
            "Debug-Mini": [200, 500, 1000, 2000, 4000],   # Small capacity  
            "Debug-Small": [500, 1000, 2000, 4000, 8000], # Medium capacity
            "Debug-Medium": [1000, 2500, 5000, 10000, 20000], # Larger capacity
        }
        return size_map.get(model_size, size_map["Debug-Small"])

#def get_scaled_dataset_sizes(model_size: str) -> List[int]:
#    """Get appropriate dataset sizes for different model scales - Morris et al. scale."""
#    # Morris et al. uses datasets from 1K to 8M+ samples (Figure 1)
#    size_map = {
#        "Tiny": [1000, 5000, 25000, 100000, 500000],
#        "Mini": [2000, 10000, 50000, 200000, 1000000], 
#        "Small": [5000, 25000, 100000, 500000, 2000000],
#        "Medium": [10000, 50000, 250000, 1000000, 4000000],
#        "Large": [25000, 100000, 500000, 2000000, 8000000],
#        "XL": [50000, 250000, 1000000, 4000000, 16000000],
#        "XXL": [100000, 500000, 2000000, 8000000, 32000000],
#    }
#    
#    return size_map.get(model_size, size_map["Small"])

def estimate_execution_time(model_config: Any, training_config: Any, dataset_sizes: List[int], device: str) -> float:
    """Estimate execution time for a model configuration."""
    
    # Base time estimates (very rough)
    base_time_per_step = {
        "cpu": 0.1,    # seconds per training step on CPU
        "cuda": 0.02   # seconds per training step on GPU
    }
    
    # Model size multiplier
    param_count = model_config.estimate_parameters()
    size_multiplier = (param_count / 1e6) ** 0.8  # Sublinear scaling
    
    # Training time
    training_time = (training_config.max_steps * 
                    base_time_per_step[device] * 
                    size_multiplier)
    
    # Capacity estimation time (multiple training runs)
    capacity_time = training_time * len(dataset_sizes) * 2  # 2 seeds
    
    # Add overhead
    total_time = (training_time + capacity_time) * 1.5
    
    return total_time


def test_progressive_scaling(viable_configs: List[Tuple], device: str, constraints: Dict) -> Dict[str, Any]:
    """Test models with progressive scaling."""
    print("\n" + "=" * 60)
    print("PROGRESSIVE SCALING TEST")
    print("=" * 60)
    
    results = {
        'successful_configs': [],
        'failed_configs': [],
        'scaling_data': [],
        'best_bits_per_param': 0.0,
        'execution_times': {}
    }
    
    for i, (model_config, model_name, estimated_params) in enumerate(viable_configs):
        print(f"\n--- Testing {model_name} Model (~{estimated_params:,} parameters) ---")
        
        # Create appropriate training config and dataset sizes
        training_config = create_scaled_training_config(model_name, device)
        dataset_sizes = get_scaled_dataset_sizes(model_name)
        
        # Estimate execution time
        estimated_time = estimate_execution_time(model_config, training_config, dataset_sizes, device)
        print(f"Estimated execution time: {estimated_time/60:.1f} minutes")
        
        # Resource check before starting
        current_memory = psutil.virtual_memory().percent
        if current_memory > 85:
            print(f"⚠ High memory usage ({current_memory:.1f}%), skipping larger models")
            results['failed_configs'].append({
                'name': model_name,
                'reason': 'High memory usage',
                'estimated_params': estimated_params
            })
            break
        
        try:
            # Test model creation first
            print("Creating model...")
            start_time = time.time()
            
            from model_trainer import create_gpt_model
            model = create_gpt_model(model_config).to(device)
            
            actual_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            print(f"✓ Model created: {actual_params:,} parameters (estimated: {estimated_params:,})")
            
            # Test capacity estimation
            print("Running capacity estimation...")
            
            try:
                from capacity_estimator import estimate_model_capacity
                
                capacity_result = estimate_model_capacity(
                    model_config=model_config,
                    training_config=training_config,
                    dataset_sizes=dataset_sizes,
                    n_seeds=2,  # Reduced for speed
                    device=device,
                    plateau_tolerance=0.05
                )
                
                if hasattr(capacity_result, 'estimated_capacity_bits'):
                    capacity_bits = capacity_result.estimated_capacity_bits
                    bits_per_param = capacity_result.bits_per_parameter
                    
                    execution_time = time.time() - start_time
                    
                    print(f"✓ Capacity: {capacity_bits:.1f} bits")
                    print(f"✓ Bits/param: {bits_per_param:.3f}")
                    print(f"✓ Execution time: {execution_time/60:.1f} minutes")
                    
                    # Store results
                    result_data = {
                        'name': model_name,
                        'estimated_params': estimated_params,
                        'actual_params': actual_params,
                        'capacity_bits': capacity_bits,
                        'bits_per_param': bits_per_param,
                        'execution_time': execution_time,
                        'dataset_sizes': dataset_sizes,
                        'memorization_values': capacity_result.memorization_values
                    }
                    
                    results['successful_configs'].append(result_data)
                    results['scaling_data'].append((actual_params, bits_per_param))
                    results['execution_times'][model_name] = execution_time
                    
                    if bits_per_param > results['best_bits_per_param']:
                        results['best_bits_per_param'] = bits_per_param
                    
                    # Clean up memory
                    del model
                    if device == "cuda":
                        import torch
                        torch.cuda.empty_cache()
                
                else:
                    print(f"✗ Capacity estimation returned unexpected format")
                    results['failed_configs'].append({
                        'name': model_name,
                        'reason': 'Unexpected capacity result format',
                        'estimated_params': estimated_params
                    })
                    
            except Exception as capacity_error:
                print(f"✗ Capacity estimation failed: {type(capacity_error).__name__}: {capacity_error}")
                results['failed_configs'].append({
                    'name': model_name,
                    'reason': f'Capacity estimation error: {capacity_error}',
                    'estimated_params': estimated_params
                })
                
        except Exception as model_error:
            print(f"✗ Model creation failed: {type(model_error).__name__}: {model_error}")
            results['failed_configs'].append({
                'name': model_name,
                'reason': f'Model creation error: {model_error}',
                'estimated_params': estimated_params
            })
            
            # If model creation fails, likely too large - stop here
            if "memory" in str(model_error).lower() or "out of memory" in str(model_error).lower():
                print("⚠ Memory issues detected, stopping progressive scaling")
                break
    
    return results


def analyze_scaling_results(results: Dict[str, Any]) -> Dict[str, Any]:
    """Analyze scaling results and compute scaling laws."""
    print("\n" + "=" * 60)
    print("SCALING ANALYSIS")
    print("=" * 60)
    
    successful = results['successful_configs']
    
    if len(successful) == 0:
        print("✗ No successful configurations to analyze")
        return {'analysis_successful': False}
    
    print(f"Analyzing {len(successful)} successful configurations:")
    
    analysis = {
        'analysis_successful': True,
        'n_models': len(successful),
        'param_range': (
            min(config['actual_params'] for config in successful),
            max(config['actual_params'] for config in successful)
        ),
        'bpp_range': (
            min(config['bits_per_param'] for config in successful),
            max(config['bits_per_param'] for config in successful)
        ),
        'best_result': max(successful, key=lambda x: x['bits_per_param']),
        'scaling_trend': None
    }
    
    # Print individual results
    for config in successful:
        print(f"  {config['name']:8}: {config['actual_params']:8,} params → {config['bits_per_param']:.3f} bits/param")
    
    # Analyze scaling trend if we have multiple points
    if len(successful) >= 2:
        import numpy as np
        
        params = np.array([config['actual_params'] for config in successful])
        bpp = np.array([config['bits_per_param'] for config in successful])
        
        # Fit log-linear relationship
        log_params = np.log10(params)
        
        # Simple linear regression in log space
        A = np.vstack([log_params, np.ones(len(log_params))]).T
        slope, intercept = np.linalg.lstsq(A, bpp, rcond=None)[0]
        
        # Calculate R²
        bpp_pred = slope * log_params + intercept
        ss_res = np.sum((bpp - bpp_pred) ** 2)
        ss_tot = np.sum((bpp - np.mean(bpp)) ** 2)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        
        analysis['scaling_trend'] = {
            'slope': slope,
            'intercept': intercept,
            'r_squared': r_squared,
            'scaling_law': f"bits/param = {slope:.3f} * log10(params) + {intercept:.3f}"
        }
        
        print(f"\nScaling law: {analysis['scaling_trend']['scaling_law']}")
        print(f"R²: {r_squared:.3f}")
        
        # Predict what we'd need for Morris target
        morris_target = 3.6
        if slope > 0:
            target_log_params = (morris_target - intercept) / slope
            target_params = 10 ** target_log_params
            print(f"Predicted parameters needed for 3.6 bits/param: {target_params:.0f}")
    
    return analysis


def create_scaling_report(
    results: Dict[str, Any], 
    analysis: Dict[str, Any], 
    constraints: Dict[str, Any],
    total_execution_time: float
) -> str:
    """Generate comprehensive scaling test report."""
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report = f"""
{'=' * 80}
MORRIS REPRODUCTION PROGRESSIVE SCALING TEST REPORT
{'=' * 80}
Test Date: {timestamp}
Total Execution Time: {total_execution_time/60:.1f} minutes

SYSTEM CONFIGURATION:
Device: {constraints['device']}
GPU Memory: {constraints['gpu_memory_gb']:.1f} GB
Available System Memory: {constraints['cpu_memory_available_gb']:.1f} GB
Max Estimated Parameters: {constraints['recommended_max_params']:,}

SCALING TEST RESULTS:
Successful Models: {len(results['successful_configs'])}
Failed Models: {len(results['failed_configs'])}
Best Bits/Parameter: {results['best_bits_per_param']:.3f}

SUCCESSFUL MODEL CONFIGURATIONS:
"""
    
    for config in results['successful_configs']:
        report += f"""
{config['name']} Model:
  Parameters: {config['actual_params']:,}
  Capacity: {config['capacity_bits']:.1f} bits
  Bits/Parameter: {config['bits_per_param']:.3f}
  Execution Time: {config['execution_time']/60:.1f} minutes
  Dataset Sizes: {config['dataset_sizes']}
"""
    
    if results['failed_configs']:
        report += f"""
FAILED CONFIGURATIONS:
"""
        for config in results['failed_configs']:
            report += f"  {config['name']}: {config['reason']}\n"
    
    if analysis.get('scaling_trend'):
        trend = analysis['scaling_trend']
        report += f"""
SCALING LAW ANALYSIS:
Relationship: {trend['scaling_law']}
R²: {trend['r_squared']:.3f}
Quality: {'Good' if trend['r_squared'] > 0.8 else 'Moderate' if trend['r_squared'] > 0.5 else 'Poor'}

Morris et al. Target: 3.6 bits/parameter
Current Best: {results['best_bits_per_param']:.3f} bits/parameter
Progress: {results['best_bits_per_param']/3.6:.1%} of Morris target
"""
    
    report += f"""
CONCLUSIONS:
"""
    
    if len(results['successful_configs']) >= 2 and results['best_bits_per_param'] > 0.1:
        report += f"""✓ Progressive scaling successful
✓ Clear scaling trend observed
✓ Larger models show improved bits/parameter ratios
✓ Pipeline validated across multiple model sizes
{'✓ Approaching Morris target range' if results['best_bits_per_param'] > 1.0 else '→ Need larger models to approach Morris 3.6 bits/param target'}

Next steps: Scale to even larger models (GPT-2/GPT-3 size) for full Morris reproduction.
"""
    else:
        report += f"""⚠ Limited scaling range achieved
- Resource constraints limited maximum model size
- Consider GPU with more memory for larger models
- Current results validate methodology at achieved scale
"""
    
    report += f"\n{'=' * 80}\n"
    return report


def main():
    """Main progressive scaling test execution."""
    print("MORRIS ET AL. PROGRESSIVE SCALING TEST")
    print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = time.time()
    
    try:
        # Step 1: Detect system capabilities
        device, constraints = detect_device_and_constraints()
        
        # Step 2: Create progressive model configurations  
        viable_configs = create_progressive_model_configs(constraints)
        
        if not viable_configs:
            print("✗ No viable model configurations found")
            return
        
        print(f"\nTesting {len(viable_configs)} configurations progressively...")
        
        # Step 3: Run progressive scaling test
        results = test_progressive_scaling(viable_configs, device, constraints)
        
        # Step 4: Analyze scaling results
        analysis = analyze_scaling_results(results)
        
        # Step 5: Generate comprehensive report
        total_execution_time = time.time() - start_time
        report = create_scaling_report(results, analysis, constraints, total_execution_time)
        
        # Save report
        report_path = f"scaling_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_path, 'w') as f:
            f.write(report)
        
        # Print final report
        print("\n" + report)
        print(f"Detailed report saved to: {report_path}")
        
        # Final assessment
        if len(results['successful_configs']) >= 2 and results['best_bits_per_param'] > 0.1:
            print("\n🎉 PROGRESSIVE SCALING: SUCCESS")
            print(f"Achieved {results['best_bits_per_param']:.3f} bits/param with largest model")
            print(f"Progress toward Morris 3.6 target: {results['best_bits_per_param']/3.6:.1%}")
        elif len(results['successful_configs']) >= 1:
            print("\n✅ SCALING TEST: PARTIAL SUCCESS")
            print("Some larger models tested successfully")
            print("Consider systems with more resources for further scaling")
        else:
            print("\n❌ SCALING TEST: LIMITED")
            print("System constraints prevented larger model testing")
            
    except Exception as e:
        print(f"\n💥 SCALING TEST: CRITICAL FAILURE")
        print(f"Unexpected error: {type(e).__name__}: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()
