{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Morris et al. Memorization Reproduction - Colab GPU\n",
        "\n",
        "Transfer of modular memorization pipeline to Google Colab for GPU acceleration.\n",
        "\n",
        "Original paper: \"How much do language models memorize?\" (Morris et al., 2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"Running on CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/mllrjo/MorrisRepro.git\n",
        "%cd MorrisRepro/memorization_reproduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "detect_dependencies"
      },
      "outputs": [],
      "source": [
        "# Auto-detect dependencies from Python files\n",
        "import re\n",
        "import os\n",
        "from collections import set\n",
        "\n",
        "def extract_imports(directory):\n",
        "    \"\"\"Extract all import statements from Python files\"\"\"\n",
        "    imports = set()\n",
        "    \n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        # Skip __pycache__ directories\n",
        "        dirs[:] = [d for d in dirs if d != '__pycache__']\n",
        "        \n",
        "        for file in files:\n",
        "            if file.endswith('.py'):\n",
        "                filepath = os.path.join(root, file)\n",
        "                try:\n",
        "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                        content = f.read()\n",
        "                        \n",
        "                    # Extract import statements\n",
        "                    import_patterns = [\n",
        "                        r'^import\\s+([\\w\\.]+)',\n",
        "                        r'^from\\s+([\\w\\.]+)\\s+import',\n",
        "                    ]\n",
        "                    \n",
        "                    for pattern in import_patterns:\n",
        "                        matches = re.findall(pattern, content, re.MULTILINE)\n",
        "                        for match in matches:\n",
        "                            # Get root package name\n",
        "                            root_package = match.split('.')[0]\n",
        "                            if root_package not in ['src', 'tests', 'os', 'sys', 're', 'json', 'math', 'random', 'time', 'datetime', 'collections', 'itertools', 'functools', 'pathlib']:\n",
        "                                imports.add(root_package)\n",
        "                                \n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {filepath}: {e}\")\n",
        "    \n",
        "    return sorted(list(imports))\n",
        "\n",
        "# Extract dependencies\n",
        "detected_imports = extract_imports('.')\n",
        "print(\"Detected imports:\")\n",
        "for imp in detected_imports:\n",
        "    print(f\"  {imp}\")\n",
        "\n",
        "# Common ML package mappings\n",
        "package_mapping = {\n",
        "    'torch': 'torch',\n",
        "    'torchvision': 'torchvision', \n",
        "    'transformers': 'transformers',\n",
        "    'numpy': 'numpy',\n",
        "    'np': 'numpy',\n",
        "    'pandas': 'pandas',\n",
        "    'pd': 'pandas',\n",
        "    'matplotlib': 'matplotlib',\n",
        "    'plt': 'matplotlib',\n",
        "    'seaborn': 'seaborn',\n",
        "    'sklearn': 'scikit-learn',\n",
        "    'tqdm': 'tqdm',\n",
        "    'wandb': 'wandb',\n",
        "    'tensorboard': 'tensorboard'\n",
        "}\n",
        "\n",
        "install_packages = []\n",
        "for imp in detected_imports:\n",
        "    if imp in package_mapping:\n",
        "        install_packages.append(package_mapping[imp])\n",
        "    else:\n",
        "        install_packages.append(imp)\n",
        "\n",
        "install_packages = list(set(install_packages))  # Remove duplicates\n",
        "print(f\"\\nPackages to install: {install_packages}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install detected dependencies\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install package with error handling\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "        print(f\"✓ Successfully installed {package}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"✗ Failed to install {package}: {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Install each package\n",
        "for package in install_packages:\n",
        "    install_package(package)\n",
        "\n",
        "print(\"\\nDependency installation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "execution_header"
      },
      "source": [
        "## 2. Pipeline Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "verify_structure"
      },
      "outputs": [],
      "source": [
        "# Verify repository structure\n",
        "print(\"Repository structure:\")\n",
        "!find . -name \"*.py\" | head -20\n",
        "\n",
        "print(\"\\nKey files:\")\n",
        "key_files = ['test_scaled_pipeline.py', 'src/model_trainer.py', 'src/experiment_runner.py']\n",
        "for file in key_files:\n",
        "    if os.path.exists(file):\n",
        "        print(f\"✓ {file}\")\n",
        "    else:\n",
        "        print(f\"✗ {file} not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline"
      },
      "outputs": [],
      "source": [
        "# Execute the main pipeline\n",
        "print(\"Starting Morris et al. memorization reproduction pipeline...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run the scaled pipeline test\n",
        "!python test_scaled_pipeline.py\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Pipeline execution complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_header"
      },
      "source": [
        "## 3. Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_results"
      },
      "outputs": [],
      "source": [
        "# Display results structure\n",
        "print(\"Results directory structure:\")\n",
        "if os.path.exists('results'):\n",
        "    !find results -type f | head -20\n",
        "else:\n",
        "    print(\"No results directory found\")\n",
        "\n",
        "print(\"\\nGenerated files:\")\n",
        "for ext in ['*.png', '*.txt', '*.json', '*.csv']:\n",
        "    files = !find . -name \"{ext}\" -not -path \"*/__pycache__/*\" | head -10\n",
        "    if files:\n",
        "        print(f\"\\n{ext} files:\")\n",
        "        for f in files:\n",
        "            print(f\"  {f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "display_plots"
      },
      "outputs": [],
      "source": [
        "# Display generated plots\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from IPython.display import Image, display\n",
        "import glob\n",
        "\n",
        "# Find and display PNG files\n",
        "png_files = glob.glob('**/*.png', recursive=True)\n",
        "png_files = [f for f in png_files if '__pycache__' not in f]\n",
        "\n",
        "print(f\"Found {len(png_files)} plot files:\")\n",
        "\n",
        "for i, png_file in enumerate(png_files[:6]):  # Limit to first 6 plots\n",
        "    print(f\"\\n{i+1}. {png_file}\")\n",
        "    try:\n",
        "        display(Image(png_file))\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying {png_file}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_reports"
      },
      "outputs": [],
      "source": [
        "# Display text reports\n",
        "import glob\n",
        "\n",
        "# Find and display recent report files\n",
        "report_files = glob.glob('*report*.txt') + glob.glob('results/**/*.txt', recursive=True)\n",
        "\n",
        "print(f\"Found {len(report_files)} report files:\")\n",
        "\n",
        "for report_file in report_files[:3]:  # Show first 3 reports\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Report: {report_file}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    try:\n",
        "        with open(report_file, 'r') as f:\n",
        "            content = f.read()\n",
        "            # Show first 2000 characters\n",
        "            if len(content) > 2000:\n",
        "                print(content[:2000])\n",
        "                print(f\"\\n... (truncated, full file has {len(content)} characters)\")\n",
        "            else:\n",
        "                print(content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {report_file}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_header"
      },
      "source": [
        "## 4. Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "package_results"
      },
      "outputs": [],
      "source": [
        "# Package results for download\n",
        "import zipfile\n",
        "import datetime\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "zip_filename = f\"morris_repro_results_{timestamp}.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # Add all result files\n",
        "    for ext in ['*.png', '*.txt', '*.json', '*.csv']:\n",
        "        files = glob.glob(f'**/{ext}', recursive=True)\n",
        "        files = [f for f in files if '__pycache__' not in f]\n",
        "        \n",
        "        for file in files:\n",
        "            zipf.write(file)\n",
        "            print(f\"Added to zip: {file}\")\n",
        "\n",
        "print(f\"\\nResults packaged in: {zip_filename}\")\n",
        "print(f\"File size: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB\")\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_header"
      },
      "source": [
        "## 5. Execution Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "execution_summary"
      },
      "outputs": [],
      "source": [
        "# Execution summary\n",
        "print(\"Morris et al. Memorization Reproduction - Execution Summary\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"✓ Repository cloned from: https://github.com/mllrjo/MorrisRepro.git\")\n",
        "print(f\"✓ Dependencies auto-detected and installed\")\n",
        "print(f\"✓ GPU status: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  - Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  - Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(f\"✓ Pipeline executed: test_scaled_pipeline.py\")\n",
        "\n",
        "# Count generated files\n",
        "result_counts = {}\n",
        "for ext in ['png', 'txt', 'json', 'csv']:\n",
        "    files = glob.glob(f'**/*.{ext}', recursive=True)\n",
        "    files = [f for f in files if '__pycache__' not in f]\n",
        "    result_counts[ext] = len(files)\n",
        "\n",
        "print(f\"✓ Results generated:\")\n",
        "for ext, count in result_counts.items():\n",
        "    if count > 0:\n",
        "        print(f\"  - {count} {ext.upper()} files\")\n",
        "\n",
        "print(f\"✓ Results packaged for download\")\n",
        "\n",
        "print(\"\\nPipeline transfer to Colab GPU complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
